{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60d8296c-c3a5-42f2-be88-68b877145ac5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m df_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m bcsc_paths:\n\u001b[1;32m---> 11\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(bcsc_paths)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1895\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1894\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "bcsc_paths = [\n",
    "    \"../data/bcsc_risk_factors_summarized1_092020.csv\",\n",
    "    \"../data/bcsc_risk_factors_summarized2_092020.csv\",\n",
    "    \"../data/bcsc_risk_factors_summarized3_092020.csv\"\n",
    "]\n",
    "\n",
    "df_list = []\n",
    "for path in bcsc_paths:\n",
    "    df = pd.read_csv(bcsc_paths)\n",
    "    print(f\"Loaded {path}, shape: {df.shape}\")\n",
    "    df_list.append(df)\n",
    "\n",
    "bcsc_df = pd.concat(df_list, ignore_index=True)\n",
    "bcsc_df_clean = bcsc_df[bcsc_df['breast_cancer_history'] != 9].reset_index(drop=True)\n",
    "print(f\"DataFrame shape after dropping: {bcsc_df_clean.shape}\")\n",
    "\n",
    "# Count how many rows dropped\n",
    "dropped_count = bcsc_df.shape[0] - bcsc_df_clean.shape[0]\n",
    "print(f\"Dropped rows: {dropped_count}\")\n",
    "\n",
    "# 3. Save cleaned concatenated DataFrame\n",
    "df_clean_path = \"../data/bcsc_concatenated_no_hist9.csv\"\n",
    "bcsc_df_clean.to_csv(df_clean_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c67743-6c6c-4485-ae50-fa2e6544d60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bcsc_concatenated_no_hist9.csv')\n",
    "print(risk_factors.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696b515-f880-41c5-93dc-f0a69be9f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "for c in [\"count\", \"year\"]:\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=[c], inplace=True)\n",
    "        print(f\"Dropped column: {c}\")\n",
    "\n",
    "# 4) Filter out 'Unknown' target values -> breast_cancer_history == 9\n",
    "if 'breast_cancer_history' not in df.columns:\n",
    "    raise KeyError(\"Target column 'breast_cancer_history' not found.\")\n",
    "initial_count = df.shape[0]\n",
    "df = df[df['breast_cancer_history'] != 9].reset_index(drop=True)\n",
    "filtered_count = df.shape[0]\n",
    "print(f\"Filtered out {initial_count - filtered_count} rows with breast_cancer_history == 9.\")\n",
    "\n",
    "# 5) Separate features (X) and target (y)\n",
    "X = df.drop(columns=[\"breast_cancer_history\"])\n",
    "\n",
    "y = df[\"breast_cancer_history\"]\n",
    "\n",
    "# 6) Handle missing values\n",
    "for col in X.columns:\n",
    "    if X[col].isnull().any():\n",
    "        if X[col].dtype in [np.float64, np.int64]:\n",
    "            median_val = X[col].median()\n",
    "            X[col] = X[col].fillna(median_val)\n",
    "            print(f\"Filled missing numeric column '{col}' with median = {median_val}\")\n",
    "        else:\n",
    "            mode_vals = X[col].mode()\n",
    "            fill_val = mode_vals[0] if not mode_vals.empty else \"Unknown\"\n",
    "            X[col] = X[col].fillna(fill_val)\n",
    "            print(f\"Filled missing categorical column '{col}' with mode = {fill_val}\")\n",
    "\n",
    "# 7) Encode categorical features\n",
    "feature_encoders = {}\n",
    "for col in X.select_dtypes(include=[\"object\", \"category\"]).columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col].astype(str))\n",
    "    feature_encoders[col] = le\n",
    "    print(f\"Encoded '{col}' with classes: {list(le.classes_)}\")\n",
    "\n",
    "# 8) Encode target if needed (likely numeric 0 or 1, so skip if numeric)\n",
    "if y.dtype == 'object' or str(y.dtype).startswith('category'):\n",
    "    target_le = LabelEncoder()\n",
    "    y = target_le.fit_transform(y.astype(str))\n",
    "    print(f\"Encoded target 'breast_cancer_history' with classes: {list(target_le.classes_)}\")\n",
    "else:\n",
    "    target_le = None\n",
    "\n",
    "# 9) Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y if len(np.unique(y))>1 else None\n",
    ")\n",
    "print(f\"Training shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
    "\n",
    "# 10) Train XGBoost classifier\n",
    "model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training finished.\")\n",
    "\n",
    "# 11) Evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "try:\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    print(f\"ROC AUC: {auc:.4f}\")\n",
    "except:\n",
    "    print(\"Could not compute ROC AUC (possible single-class).\")\n",
    "\n",
    "# 12) Save artifacts\n",
    "MODEL_PATH = os.path.join(DATA_DIR, \"bcsc_xgb_model.pkl\")\n",
    "ENCODERS_PATH = os.path.join(DATA_DIR, \"bcsc_feature_encoders.pkl\")\n",
    "joblib.dump(model, MODEL_PATH)\n",
    "print(f\"Saved model to {MODEL_PATH}\")\n",
    "joblib.dump(feature_encoders, ENCODERS_PATH)\n",
    "print(f\"Saved feature encoders to {ENCODERS_PATH}\")\n",
    "\n",
    "if target_le is not None:\n",
    "    TARGET_PATH = os.path.join(DATA_DIR, \"bcsc_target_encoder.pkl\")\n",
    "    joblib.dump(target_le, TARGET_PATH)\n",
    "    print(f\"Saved target encoder to {TARGET_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba1223-2d2f-48a7-92ad-7d9b011df80b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
